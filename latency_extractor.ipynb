{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace 'EXECUTION_ID' with the test run id from output of this command \n",
    "`opensearch-benchmark list test_executions --limit 100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "EXECUTION_ID = \"60139469-ef0b-4a1e-9198-c58977ee254c\"\n",
    "PATH = \".benchmark/benchmarks/test_executions/{}/test_execution.json\".format(EXECUTION_ID)\n",
    "\n",
    "user_path = os.path.expanduser(\"~\")\n",
    "benchmark_path = os.path.join(user_path, PATH)\n",
    "\n",
    "print(\"INFO: try to load file at '{}'\".format(benchmark_path))\n",
    "with open(benchmark_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "if not 'results' in data:\n",
    "    print(f\"ERROR: Benchmark file has no 'result' section.\")\n",
    "    exit(1)\n",
    "\n",
    "op_metrics = data['results']['op_metrics']\n",
    "\n",
    "for metric in op_metrics:\n",
    "    if 'latency' in metric and \\\n",
    "       '50_0' in metric['latency'] and \\\n",
    "       '90_0' in metric['latency']:\n",
    "        print(\"latency.50_0\\t{}\\t{}\".format(int(metric[\"latency\"][\"50_0\"]), metric[\"operation\"]))\n",
    "        print(\"latency.90_0\\t{}\\t{}\".format(int(metric[\"latency\"][\"90_0\"]), metric[\"operation\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
